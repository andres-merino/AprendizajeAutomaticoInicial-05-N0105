\documentclass[a4,11pt]{aleph-notas}

% -- Paquetes adicionales 
\usepackage{enumitem}
\usepackage{url}
\usepackage{array}
\usepackage{booktabs}
\hypersetup{
    urlcolor=blue,
    linkcolor=blue,
}

% -- Datos 
\institucion{Facultad de Ciencias Exactas, Naturales y Ambientales}
\carrera{Ciencia de Datos}
\asignatura{Aprendizaje Automático Inicial}
\tema{Clase 14: Perceptrón multicapa}
\autor{Andrés Merino}
\fecha{Periodo 2025-2}

\logouno[0.14\textwidth]{Logos/logoPUCE_04_ac}
\definecolor{colortext}{HTML}{0030A1}
\definecolor{colordef}{HTML}{0030A1}
\fuente{montserrat}

\begin{document}

\encabezado
% En todo el documento, las indicaciones deben ser simples y directas, con una sola oración.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Resultado de Aprendizaje}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{RdA de la asignatura:}
% Se toma uno de los siguientes
\begin{itemize}[leftmargin=*]
    \item \textbf{RdA 2:} Aplicar modelos de aprendizaje automático supervisado y no supervisado, así como su validación y optimización, en la resolución de problemas tanto reales como simulados.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{RdA de la clase:}
% Máximo 3 resultados
\begin{itemize}[leftmargin=*]
    \item Comprender las arquitecturas básicas de redes neuronales, enfocándose en el Perceptrón Multicapa.
    \item Implementar el Perceptrón Multicapa en problemas de regresión, clasificación binaria y multiclase.
    \item Aplicar el Perceptrón Multicapa en un caso práctico para resolver un problema real.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Introducción}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Pregunta inicial:} 
Si una sola neurona no es capaz de captar patrones complejos, ¿qué cambia cuando varias neuronas se organizan y cooperan entre sí?

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Desarrollo}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Actividad 1: Introducción a las arquitecturas de redes neuronales}

En esta actividad se presentará mediante clase magistral la arquitectura básica del Perceptrón Multicapa, explicando su estructura de capas (entrada, ocultas, salida), ventajas sobre el perceptrón simple y capacidad para resolver problemas no linealmente separables.

\paragraph{¿Cómo lo haremos?}  
\begin{itemize}[leftmargin=*]
    \item \textbf{Clase magistral:} 
    Presentación teórica sobre:
    \begin{itemize}
        \item Arquitectura básica del Perceptrón Multicapa (capas de entrada, ocultas y salida).
        \item Ventajas sobre el perceptrón simple (capacidad de aprender funciones no lineales).
        \item Hiperparámetros clave (número de capas, neuronas por capa, funciones de activación).
    \end{itemize}
    
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Actividad 2: Implementación práctica del Perceptrón Multicapa}

En esta actividad los estudiantes implementarán el Perceptrón Multicapa mediante exploración de cuadernos de Jupyter para diferentes tipos de problemas: regresión, clasificación binaria, clasificación multiclase y un caso práctico real.

\paragraph{¿Cómo lo haremos?}  
\begin{itemize}[leftmargin=*]
    \item \textbf{Explicación de casos de uso:}
    Se explicará cómo el Perceptrón Multicapa se adapta a diferentes tipos de problemas mediante la configuración de la capa de salida y la función de pérdida apropiada.
    
    \item \textbf{Implementación en Python:} Los estudiantes accederán a cuadernos de Jupyter previamente preparados para cada tipo de problema.
    \begin{itemize}
        \item \textbf{Regresión:}
        \begin{quote}
            Enlace al cuaderno: \href{https://github.com/andres-merino/AprendizajeAutomaticoInicial-05-N0105/blob/main/2-Notebooks/14-1-Perceptron-Multicapa-Regresion.ipynb}{14-1-Perceptron-Multicapa-Regresion.ipynb}.
        \end{quote}
        
        \item \textbf{Clasificación Binaria:}
        \begin{quote}
            Enlace al cuaderno: \href{https://github.com/andres-merino/AprendizajeAutomaticoInicial-05-N0105/blob/main/2-Notebooks/14-2-Perceptron-Multicapa-Clasificacion.ipynb}{14-2-Perceptron-Multicapa-Clasificacion.ipynb}.
        \end{quote}
        
        \item \textbf{Clasificación Multiclase:}
        \begin{quote}
            Enlace al cuaderno: \href{https://github.com/andres-merino/AprendizajeAutomaticoInicial-05-N0105/blob/main/2-Notebooks/14-3-Perceptron-Multicapa-Multiclase.ipynb}{14-3-Perceptron-Multicapa-Multiclase.ipynb}.
        \end{quote}
        
        \item \textbf{Caso Práctico:}
        \begin{quote}
            Enlace al cuaderno: \href{https://github.com/andres-merino/AprendizajeAutomaticoInicial-05-N0105/blob/main/2-Notebooks/14-4-Perceptron-Practica.ipynb}{14-4-Perceptron-Practica.ipynb}.
        \end{quote}
    \end{itemize}
    
    \item \textbf{Experimentación:} Cambia la arquitectura de la red y las funciones de activación hasta obtener un mejor ajuste.
    
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Cierre}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{Verificación de aprendizaje:}
\begin{enumerate}[leftmargin=*]
    \item ¿Qué es el Perceptrón Multicapa y cuál es su principal ventaja sobre el perceptrón simple?
    
    \item ¿Cómo se ajustan los hiperparámetros para optimizar el modelo de Perceptrón Multicapa?
\end{enumerate}

\paragraph{Preguntas tipo entrevista:}
\begin{enumerate}[leftmargin=*]
    \item Estás entrenando un Perceptrón Multicapa y observas que el loss de entrenamiento disminuye constantemente pero el loss de validación aumenta después de cierta época. ¿Qué está ocurriendo y cómo lo solucionarías?
    
    \item Tienes que elegir entre un Perceptrón Multicapa con 2 capas ocultas de 100 neuronas cada una, o 4 capas ocultas de 50 neuronas cada una. Ambos tienen aproximadamente el mismo número de parámetros. ¿Cuál elegirías y por qué?
    % Respuesta: Depende del problema, pero generalmente preferiría la red más profunda (4 capas × 50 neuronas) por: (1) Redes profundas aprenden representaciones jerárquicas más ricas: capas iniciales detectan características simples, capas profundas combinan en abstracciones complejas, (2) Mejor capacidad de generalización teórica con mismos parámetros, (3) Permite aprender funciones más complejas. Sin embargo, consideraciones: (1) Redes más profundas son más difíciles de entrenar (vanishing/exploding gradients) - requieren batch normalization, skip connections, (2) Mayor riesgo de overfitting - necesitan más regularización, (3) Más costosas computacionalmente. Empezaría con la red más simple (2×100), validaría resultados, y solo incrementaría profundidad si los datos y el problema lo justifican. La profundidad óptima se determina experimentalmente con validación cruzada.
\end{enumerate}

\paragraph{Tarea:}
Desarrollar los ejercicios planteados en el siguiente cuaderno y entregarlo por el aula virtual:
\begin{quote}
    Enlace al cuaderno: \href{https://colab.research.google.com/github/andres-merino/AprendizajeAutomaticoInicial-05-N0105/blob/main/2-Ejercicios/07-Perceptron.ipynb}{07-Perceptron.ipynb}.
\end{quote}

\paragraph{Pregunta de investigación:}  
\begin{enumerate}[leftmargin=*]
    \item ¿Qué otras arquitecturas de redes neuronales pueden ser utilizadas para problemas similares?
    \item ¿Cómo afecta la cantidad de datos al rendimiento del Perceptrón Multicapa?
    \item ¿Cuáles son las limitaciones del Perceptrón Multicapa frente a otras arquitecturas como redes convolucionales o recurrentes?
\end{enumerate}

\paragraph{Para la próxima clase:}  
Visualizar el siguiente video sobre Retropropagación:
\begin{quote}
    Enlace al video: \href{https://youtu.be/eNIqz_noix8?si=RzQqVstBn3TFKDk7}{Backpropagation}.
\end{quote}


\end{document}